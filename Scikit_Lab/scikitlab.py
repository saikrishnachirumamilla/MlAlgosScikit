# -*- coding: utf-8 -*-
"""scikitlab_sxc180078.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VC_i0IF8TfKvyGnK52MdfV7kd0uBEYT4
"""

import pandas as pd
from numpy import array
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings
warnings.simplefilter("ignore")

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

banknote = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt', header=None, names=['variance','skewness','curtosis','entropy','class'])

X = banknote[['variance','skewness','curtosis','entropy']]
y = banknote[['class']]
y = array(y).flatten()
scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

classifiers = ["Decision Tree", "Neural Net","Support Vector Machine", "Gaussian Naive Bayes", "Logistic Regression", "k-Nearest Neighbors",
         "Bagging", "Random Forest", "AdaBoost Classifier", "Gradient Boosting Classifier", "XGBoost Classifier"]
avg_Precision=[]
avg_recall=[]
avg_F1_score=[]
accuracy_list = []
best_parameters=[]

tuned_parameters = [{'max_depth': [4,5,6],'min_samples_split':[8,9,10], 
                      'min_samples_leaf':[2,3,4],'min_impurity_decrease':[0.0001,0.0005,0.001,0.005,0.01],
                      'max_features':[2,3,4],'max_leaf_nodes':[1000,1200,1400],
                      'min_weight_fraction_leaf':[0,0.01]}]
score = 'precision'
print("# Decision Tree")
print("# Tuning hyper-parameters for %s" % score)
print()

clf = GridSearchCV(DecisionTreeClassifier(), tuned_parameters, cv=5,scoring='%s_macro' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
  print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
results = classification_report(y_true, y_pred,output_dict=True)
print(classification_report(y_true, y_pred))
print()
best_parameters.append(clf.best_params_)
avg_Precision.append(results['macro avg']['precision'])
avg_recall.append(results['macro avg']['recall'])
avg_F1_score.append(results['macro avg']['f1-score'])
accuracy_list.append(results['accuracy'])

tuned_parameters = [{'hidden_layer_sizes':[(5,5,5)],'activation':['identity','logistic','tanh','relu'],
                      'alpha':[0.1,0.5],'learning_rate':['constant','adaptive'],'max_iter':[1500], 'tol':[1e-1,1e-4],
                      'momentum':[0.9,1.0], 'early_stopping':[False,True]}]
score = 'precision'
print("# Neural Network")
print("# Tuning hyper-parameters for %s" % score)
print()

clf = GridSearchCV(MLPClassifier(), tuned_parameters, cv=5,scoring='%s_macro' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
  print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
results = classification_report(y_true, y_pred,output_dict=True)
print(classification_report(y_true, y_pred))
print()
best_parameters.append(clf.best_params_)
avg_Precision.append(results['macro avg']['precision'])
avg_recall.append(results['macro avg']['recall'])
avg_F1_score.append(results['macro avg']['f1-score'])
accuracy_list.append(results['accuracy'])

tuned_parameters = [{'kernel': ['rbf', 'linear', 'poly'], 'gamma': [1e-3, 1e-4], 
                     'C': [1, 10, 50, 100],'random_state': [2,5,10,15,20]}]
score = 'precision'
print("# Support Vector Machine")
print("# Tuning hyper-parameters for %s" % score)
print()

clf = GridSearchCV(SVC(), tuned_parameters, cv=5,scoring='%s_macro' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
  print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
results = classification_report(y_true, y_pred,output_dict=True)
print(classification_report(y_true, y_pred))
print()
best_parameters.append(clf.best_params_)
avg_Precision.append(results['macro avg']['precision'])
avg_recall.append(results['macro avg']['recall'])
avg_F1_score.append(results['macro avg']['f1-score'])
accuracy_list.append(results['accuracy'])

tuned_parameters = [{'priors': [[0.5,0.5],[0.25,0.75],[0.3,0.7],[0.6,0.4], [0.1,0.9]]}]
score = 'precision'
print("# Gaussian Naive Bayes")
print("# Tuning hyper-parameters for %s" % score)
print()

clf = GridSearchCV(GaussianNB(), tuned_parameters, cv=5,scoring='%s_macro' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
  print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
results = classification_report(y_true, y_pred,output_dict=True)
print(classification_report(y_true, y_pred))
print()
best_parameters.append(clf.best_params_)
avg_Precision.append(results['macro avg']['precision'])
avg_recall.append(results['macro avg']['recall'])
avg_F1_score.append(results['macro avg']['f1-score'])
accuracy_list.append(results['accuracy'])

tuned_parameters = [{'penalty': ['l2'], 'tol': [1e-4], 'C': [1.0],
                      'fit_intercept': [True,False], 'intercept_scaling': [1], 
                      'max_iter': [100,500], 'multi_class': ['ovr','multinomial','auto'],
                      'solver': ['newton-cg','lbfgs','sag','saga']}]
score = 'precision'
print("# Logistic Regression")
print("# Tuning hyper-parameters for %s" % score)
print()

clf = GridSearchCV(LogisticRegression(), tuned_parameters, cv=5,scoring='%s_macro' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
  print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
results = classification_report(y_true, y_pred,output_dict=True)
print(classification_report(y_true, y_pred))
print()
best_parameters.append(clf.best_params_)
avg_Precision.append(results['macro avg']['precision'])
avg_recall.append(results['macro avg']['recall'])
avg_F1_score.append(results['macro avg']['f1-score'])
accuracy_list.append(results['accuracy'])

tuned_parameters = [{'n_neighbors': [2,5,8], 
                     'algorithm': ['auto','ball_tree','kd_tree','brute'], 
                     'p': [1,3,7], 'weights': ['uniform', 'distance']}]
score = 'precision'
print("# k-Nearest Neighbors")
print("# Tuning hyper-parameters for %s" % score)
print()

clf = GridSearchCV(KNeighborsClassifier(), tuned_parameters, cv=5,scoring='%s_macro' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
  print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
results = classification_report(y_true, y_pred,output_dict=True)
print(classification_report(y_true, y_pred))
print()
best_parameters.append(clf.best_params_)
avg_Precision.append(results['macro avg']['precision'])
avg_recall.append(results['macro avg']['recall'])
avg_F1_score.append(results['macro avg']['f1-score'])
accuracy_list.append(results['accuracy'])

tuned_parameters = [{'n_estimators': [5,10,15,16,20], 'max_samples': [5,7,9,10,12,15], 
                      'max_features': [1,2,3,4], 'random_state': [5,6,7,8,9]}]
score = 'precision'
print("# Bagging")
print("# Tuning hyper-parameters for %s" % score)
print()

clf = GridSearchCV(BaggingClassifier(), tuned_parameters, cv=5,scoring='%s_macro' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
  print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
results = classification_report(y_true, y_pred,output_dict=True)
print(classification_report(y_true, y_pred))
print()
best_parameters.append(clf.best_params_)
avg_Precision.append(results['macro avg']['precision'])
avg_recall.append(results['macro avg']['recall'])
avg_F1_score.append(results['macro avg']['f1-score'])
accuracy_list.append(results['accuracy'])

tuned_parameters = [{'n_estimators': [10,15], 'max_depth': [5,10], 'max_features':[1,2,3],
                     'criterion': ['gini','entropy'], 'min_samples_split': [2,3,4], 
                     'min_samples_leaf': [1,2,3,4,5]}]
score = 'precision'
print("# Random Forest")
print("# Tuning hyper-parameters for %s" % score)
print()

clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5,scoring='%s_macro' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
  print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
results = classification_report(y_true, y_pred,output_dict=True)
print(classification_report(y_true, y_pred))
print()
best_parameters.append(clf.best_params_)
avg_Precision.append(results['macro avg']['precision'])
avg_recall.append(results['macro avg']['recall'])
avg_F1_score.append(results['macro avg']['f1-score'])
accuracy_list.append(results['accuracy'])

tuned_parameters = [{'n_estimators': [10,15], 'learning_rate': [1,2,3], 
                        'algorithm': ['SAMME', 'SAMME.R'], 'random_state': [5,10,15]}]
score = 'precision'
print("#AdaBoost Classifier")
print("# Tuning hyper-parameters for %s" % score)
print()

clf = GridSearchCV(AdaBoostClassifier(), tuned_parameters, cv=5,scoring='%s_macro' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
  print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
results = classification_report(y_true, y_pred,output_dict=True)
print(classification_report(y_true, y_pred))
print()
best_parameters.append(clf.best_params_)
avg_Precision.append(results['macro avg']['precision'])
avg_recall.append(results['macro avg']['recall'])
avg_F1_score.append(results['macro avg']['f1-score'])
accuracy_list.append(results['accuracy'])

tuned_parameters = [{'loss': ['deviance'], 'learning_rate': [0.2,0.4,0.5,0.8],
                      'n_estimators': [25,50,75,100], 'max_features': [1,2] }]
score = 'precision'
print("# Gradient Boosting Classifier")
print("# Tuning hyper-parameters for %s" % score)
print()

clf = GridSearchCV(GradientBoostingClassifier(), tuned_parameters, cv=5,scoring='%s_macro' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
  print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
results = classification_report(y_true, y_pred,output_dict=True)
print(classification_report(y_true, y_pred))
print()
best_parameters.append(clf.best_params_)
avg_Precision.append(results['macro avg']['precision'])
avg_recall.append(results['macro avg']['recall'])
avg_F1_score.append(results['macro avg']['f1-score'])
accuracy_list.append(results['accuracy'])

tuned_parameters = [{'learning_rate': [0.1,0.5,1,2], 'n_estimators': [10,15,20,25,30], 'seed': [1,2], 
                      'min_child_weight': [1,5,10,20,40,50,100], 'max_delta_step':[1,5,10,20,40,100]}]
score = 'precision'
print("# XGBoost")
print("# Tuning hyper-parameters for %s" % score)
print()

clf = GridSearchCV(XGBClassifier(), tuned_parameters, cv=5,scoring='%s_macro' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
  print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
results = classification_report(y_true, y_pred,output_dict=True)
print(classification_report(y_true, y_pred))
print()
best_parameters.append(clf.best_params_)
avg_Precision.append(results['macro avg']['precision'])
avg_recall.append(results['macro avg']['recall'])
avg_F1_score.append(results['macro avg']['f1-score'])
accuracy_list.append(results['accuracy'])

d={'Algorithm':classifiers, 'Best Parameters':best_parameters, 'Avg Precision':avg_Precision,
   'Avg Recall': avg_recall, 'Avg F1': avg_F1_score, 'Accuracy':accuracy_list}
output=pd.DataFrame(data=d)
output.style.set_properties(**{'background-color': 'white',
                           'color': 'black'})